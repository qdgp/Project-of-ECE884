{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competent-cleaner",
   "metadata": {},
   "source": [
    "# 1. Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, MaxPooling2D, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-method",
   "metadata": {},
   "source": [
    "# 2. Load and Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./archive/trainLabels_cropped.csv')\n",
    "root_dir=os.path.join('./levels')\n",
    "batch_size=64\n",
    "img_width=256\n",
    "img_height=256\n",
    "\n",
    "def image_generator(root_dir):\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                      #shear_range=0.2,\n",
    "                                      #zoom_range=0.2,\n",
    "                                      horizontal_flip=True,\n",
    "                                      vertical_flip=True,\n",
    "                                      validation_split=0.2)\n",
    "  \n",
    "    train_generator = datagen.flow_from_directory(root_dir,\n",
    "                                  target_size = (img_width, img_height),\n",
    "                                  batch_size = batch_size,\n",
    "                                  class_mode = 'categorical',\n",
    "                                  subset='training')\n",
    " \n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(root_dir,\n",
    "                                 target_size=(img_width, img_height),\n",
    "                                 batch_size = batch_size,\n",
    "                                 class_mode = 'categorical',\n",
    "                                 subset='validation')    \n",
    "    \n",
    "    return train_generator, test_generator\n",
    "\n",
    "train_generator, test_generator = image_generator(root_dir)\n",
    "divergence_fn = lambda q,p,_:tfd.kl_divergence(q,p)/train_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-float",
   "metadata": {},
   "source": [
    "# 3. Bayesian by backprop Moedels \n",
    "+ In this section we introduced 3 bayesian models \n",
    "    + Bayesian Model 1 is a Bayesian by back prop model usign the repramaterization trick The repramatrization trick is an approximative way in sloving bayesian function \n",
    "    + Bayesian Model2 is another bayesian by backprop mehtods that repalces the repramaterization trick/ repramtrization layers with the flipout layers. The flipout layers use Montecarol approximation to solve the bayesian function. \n",
    "        + one advanteage of that method is its faster training time and more weights/ more degrees of freedom.\n",
    "        \n",
    "  + Bayesain Model 3 is an imporvment over bayesian modle 2 withere the trainign proces was\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-brush",
   "metadata": {},
   "source": [
    "## Bayesian Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bayes = Sequential([    \n",
    "    tfpl.Convolution2DReparameterization(input_shape=(img_width, img_height,3),padding=\"same\",filters=8, kernel_size=16, activation='relu',\n",
    "                                           kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                           kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                           kernel_divergence_fn = divergence_fn,\n",
    "                                           bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                           bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                           bias_divergence_fn = divergence_fn),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseReparameterization(units=tfpl.OneHotCategorical.params_size(5), activation=None,\n",
    "                                    kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                    kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                    kernel_divergence_fn = divergence_fn,\n",
    "                                    bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                    bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                    bias_divergence_fn = divergence_fn\n",
    "                                ),\n",
    "    tfpl.OneHotCategorical(5)\n",
    "    \n",
    "])\n",
    "model_bayes.summary()\n",
    "\n",
    "learning_rate = 0.005\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-burton",
   "metadata": {},
   "source": [
    "## Bayesian Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bayes = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(img_width, img_height, 3),name=\"basket\"),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(16, kernel_size=5, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1a\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "\n",
    "    MaxPool2D(strides=(4,4), pool_size=(4,4), padding=\"same\"),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(32, kernel_size=3, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1b\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(4,4), pool_size=(4,4), padding=\"same\"),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseFlipout(units=5, kernel_divergence_fn=divergence_fn),\n",
    "    #tfpl.OneHotCategorical(5)\n",
    "])\n",
    "model_bayes.summary()\n",
    "\n",
    "learning_rate = 1.0e-3\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    y_pred = tfp.distributions.MultivariateNormalTriL(y_pred)\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-audit",
   "metadata": {},
   "source": [
    "## Bayesian Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bayes = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(img_width, img_height, 3),name=\"basket\"),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(16, kernel_size=5, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1a\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    tfpl.Convolution2DFlipout(32, kernel_size=3, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1b\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseFlipout(units=5,activation='softmax', kernel_divergence_fn=divergence_fn),\n",
    "    #tfpl.OneHotCategorical(5)\n",
    "])\n",
    "model_bayes.summary()\n",
    "learning_rate = 1.0e-3\n",
    "\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    y_pred = tfp.distributions.MultivariateNormalTriL(y_pred)\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-fiber",
   "metadata": {},
   "source": [
    "# 4. Compile and Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class0(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 0), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class1(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 1), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class2(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 2), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class3(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 3), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class4(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 4), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "model_bayes.compile(loss = negative_log_likelihood,\n",
    "              optimizer = Adam(learning_rate=learning_rate),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),class0,class1,class2,class3,class4],\n",
    "              experimental_run_tf_function = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_bayes.fit(train_generator,\n",
    "                                steps_per_epoch = train_generator.samples // batch_size,\n",
    "                                validation_data = test_generator, \n",
    "                                validation_steps = test_generator.samples // batch_size,\n",
    "                                epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-cannon",
   "metadata": {},
   "source": [
    "# 5. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"history_plots.npz\",history.history)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12, 3))\n",
    "fig.suptitle(\"Bayesian Model 2's accuracy and Loss\",fontsize=12)\n",
    "\n",
    "\n",
    "ax1.plot(history2.history['CategoricalAccuracy'],label='train')\n",
    "ax1.plot(history2.history['val_CategoricalAccuracy'],label='test')\n",
    "ax1.title.set_text('model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history2.history['loss'],label='train')\n",
    "ax2.plot(history2.history['val_loss'],label='test')\n",
    "ax2.title.set_text('model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_predict_bayes(image, true_label):\n",
    "    #read image\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    #show the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    img_resize = (cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC))/255.\n",
    "    predicted_probabilities = np.empty(shape=(300, 5))\n",
    "    predicted_probabilities2 = np.empty(shape=(300, 5))\n",
    "    predicted_probabilities3 = np.empty(shape=(300, 5))\n",
    "    for i in range(300):\n",
    "        predicted_probabilities2[i] =model_bayes3.predict(img_resize[np.newaxis,...])   \n",
    "    pct_2p5 = np.array([np.percentile(predicted_probabilities2[:, i], 2.5) for i in range(5)])\n",
    "    pct_97p5 = np.array([np.percentile(predicted_probabilities2[:, i], 97.5) for i in range(5)])\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bar = ax.bar(np.arange(5), pct_97p5-0.02, color='red')\n",
    "    bar[true_label].set_color('green')\n",
    "    bar = ax.bar(np.arange(5), pct_2p5-0.02, color='white')\n",
    "    ax.set_xticklabels([''] + [x for x in label],fontsize=20)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('Probability',fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "def import_and_predict2(image_data, label, T):    \n",
    "    #read image\n",
    "    img = cv2.imread(image_data)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    #show the image\n",
    "    #plt.imshow(img)\n",
    "    #plt.axis('off')\n",
    "    # resize and reshape the image\n",
    "    img_resize = (cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC))/255.\n",
    "    img_reshape = img_resize[np.newaxis,...]\n",
    "    #predict the image\n",
    "    prediction = model_bayes3.predict(img_reshape)    \n",
    "    p_hat=[]\n",
    "    for t in range (T):\n",
    "        p_hat.append(model_bayes3.predict(img_reshape)[0])\n",
    "    p_hat=np.array(p_hat)\n",
    "    prediction = np.mean(p_hat, axis=0)\n",
    "    label_prediction = label[np.argmax(prediction)]\n",
    "    aleatoric = np.mean(p_hat*(1-p_hat), axis=0)\n",
    "    epistemic = np.mean(p_hat**2, axis=0) - np.mean(p_hat, axis=0)**2\n",
    "    label_prediction = [np.argmax(prediction)]\n",
    "    return prediction, label_prediction, aleatoric, epistemic\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = os.listdir(root_dir)\n",
    "image0_dir = os.path.join(root_dir+'/0/20_right.jpeg')\n",
    "image0_dir2 = os.path.join(root_dir+'/0/70_left.jpeg')\n",
    "\n",
    "image1_dir = os.path.join(root_dir+'/1/2161_left.jpeg')\n",
    "image2_dir = os.path.join(root_dir+'/2/1034_left.jpeg')\n",
    "image3_dir = os.path.join(root_dir+'/3/13489_left.jpeg')\n",
    "image4_dir = os.path.join(root_dir+'/4/5032_right.jpeg')\n",
    "print(root_dir)\n",
    "\n",
    "prediction = import_and_predict_bayes(image0_dir2,  label.index('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=root_dir\n",
    "Class_Names=[\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "total_prediction=np.empty(5)\n",
    "total_label_prediction=np.empty(1)\n",
    "total_aleatoric=np.empty(5)\n",
    "total_epistemic=np.empty(5)\n",
    "total_accuracy=np.empty(1)\n",
    "total_tracker=np.empty(1)\n",
    "tracker=np.empty(1)\n",
    "\n",
    "for Class in Class_Names:\n",
    "    path = os.path.join(input_dir,Class)\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        T=10 # T should be 2 or greater\n",
    "        \n",
    "        image_dir=os.path.join(path,file)\n",
    "        #print(image_dir)\n",
    "        prediction, label_prediction, aleatoric, epistemic=import_and_predict2(image_dir, label,T)                        \n",
    "        total_prediction=np.vstack((total_prediction,prediction))\n",
    "        total_label_prediction=np.vstack((total_label_prediction,label_prediction))\n",
    "        total_aleatoric=np.vstack((total_aleatoric,aleatoric))\n",
    "        total_epistemic=np.vstack((total_epistemic,epistemic))\n",
    "        tracker=Class_Names.index(Class)\n",
    "        total_tracker=np.vstack((total_tracker,tracker))\n",
    "                            \n",
    "        if(int(\"\".join(map(str, label_prediction)))==Class_Names.index(Class)):\n",
    "                total_accuracy=np.vstack((total_accuracy,1))\n",
    "        else: \n",
    "                total_accuracy=np.vstack((total_accuracy,0))\n",
    "\n",
    "total_prediction1=total_prediction[1:,:]\n",
    "total_label_prediction1=total_label_prediction[1:,:]\n",
    "total_aleatoric1=total_aleatoric[1:,:]\n",
    "total_epistemic1=total_epistemic[1:,:]\n",
    "total_accuracy1=total_accuracy[1:,:]\n",
    "total_tracker1=total_tracker[1:,:]                \n",
    "np.savez(\"dataDR_bayes3_700.npz\",total_prediction1,total_label_prediction1,total_aleatoric1,total_epistemic1,total_accuracy1,total_tracker1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
