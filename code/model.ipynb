{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "competent-cleaner",
   "metadata": {},
   "source": [
    "# 1. Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this section we import the packages used \n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, MaxPooling2D, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-method",
   "metadata": {},
   "source": [
    "# 2. Load and Generate Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('./trainLabels_cropped.csv')\n",
    "root_dir=os.path.join('./levels')\n",
    "batch_size=64\n",
    "img_width=256\n",
    "img_height=256\n",
    "\n",
    "# use image generator to import and modulate the images \n",
    "# Image generator also splits the data in the training dataset and the test dataset \n",
    "\n",
    "def image_generator(root_dir):\n",
    "    \n",
    "    datagen = ImageDataGenerator(rescale=1/255,\n",
    "                                      #shear_range=0.2,\n",
    "                                      #zoom_range=0.2,\n",
    "                                      horizontal_flip=True,\n",
    "                                      vertical_flip=True,\n",
    "                                      validation_split=0.2)\n",
    "  \n",
    "    train_generator = datagen.flow_from_directory(root_dir,\n",
    "                                  target_size = (img_width, img_height),\n",
    "                                  batch_size = batch_size,\n",
    "                                  class_mode = 'categorical',\n",
    "                                  subset='training')\n",
    " \n",
    "    \n",
    "    test_generator = datagen.flow_from_directory(root_dir,\n",
    "                                 target_size=(img_width, img_height),\n",
    "                                 batch_size = batch_size,\n",
    "                                 class_mode = 'categorical',\n",
    "                                 subset='validation')    \n",
    "    \n",
    "    return train_generator, test_generator\n",
    "\n",
    "# KL divergence funcction below uses the lambda function to pass the an \n",
    "# input to the Kernel_divergence of either the filpout layer or the reprametrization layer\n",
    "\n",
    "train_generator, test_generator = image_generator(root_dir)\n",
    "divergence_fn = lambda q,p,_:tfd.kl_divergence(q,p)/train_generator.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-float",
   "metadata": {},
   "source": [
    "# 3. Bayesian by backprop Moedels \n",
    "In this section we introduced 3 bayesian models:\n",
    "+ Bayesian Model 1 is a Bayesian by back prop model using the reparameterization trick the reparameterization trick is an approximative way in solving Bayesian function\n",
    "+ Bayesian Model2 is another Bayesian by backprop methods that replaces the reparameterization trick/ reparameterization layers with the flipout layers. The flipout layers use Mont carol approximation to solve the Bayesian function. One advantage of that method is its faster training time and more weights/ more degrees of freedom.\n",
    "+ Bayesian Model 3 is an improvement over Bayesian model 2 with the training process with different layer shapes and different filter sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-brush",
   "metadata": {},
   "source": [
    "## Bayesian Model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 below is the deepest modle it uses reprametrization layers to implement bayes by back prop \n",
    "\n",
    "model_bayes = Sequential([    \n",
    "    tfpl.Convolution2DReparameterization(input_shape=(img_width, img_height,3),padding=\"same\",filters=8, kernel_size=16, activation='relu',\n",
    "                                           kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                           kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                           kernel_divergence_fn = divergence_fn,\n",
    "                                           bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                           bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                           bias_divergence_fn = divergence_fn),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    Conv2D(256, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseReparameterization(units=tfpl.OneHotCategorical.params_size(5), activation=None,\n",
    "                                    kernel_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                    kernel_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                    kernel_divergence_fn = divergence_fn,\n",
    "                                    bias_prior_fn = tfpl.default_multivariate_normal_fn,\n",
    "                                    bias_posterior_fn=tfpl.default_mean_field_normal_fn(is_singular=False),\n",
    "                                    bias_divergence_fn = divergence_fn\n",
    "                                ),\n",
    "    tfpl.OneHotCategorical(5)\n",
    "    \n",
    "])\n",
    "model_bayes.summary()\n",
    "\n",
    "learning_rate = 0.005\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-burton",
   "metadata": {},
   "source": [
    "## Bayesian Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 is similar to modle 1 but replaces the repramaterization layers with filpout layers \n",
    "# filpout layers use monte carol approximation as a method to control the variance\n",
    "# flip out layers introduce more parameters (almost double) compared to the previously mentioned reprameterization layers \n",
    "# flipout layers have been noteiced to train faster \n",
    "\n",
    "model_bayes = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(img_width, img_height, 3),name=\"basket\"),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(16, kernel_size=5, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1a\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "\n",
    "    MaxPool2D(strides=(4,4), pool_size=(4,4), padding=\"same\"),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(32, kernel_size=3, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1b\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(4,4), pool_size=(4,4), padding=\"same\"),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseFlipout(units=5, kernel_divergence_fn=divergence_fn),\n",
    "])\n",
    "model_bayes.summary()\n",
    "\n",
    "learning_rate = 1.0e-3\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    y_pred = tfp.distributions.MultivariateNormalTriL(y_pred)\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-audit",
   "metadata": {},
   "source": [
    "## Bayesian Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model bayesian 3 is an imporvment upon Bayesain model 2 \n",
    "# where the layer sizes have been adjusted to provide the best results\n",
    "\n",
    "model_bayes = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(img_width, img_height, 3),name=\"basket\"),\n",
    "    \n",
    "    tfpl.Convolution2DFlipout(16, kernel_size=5, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1a\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    tfpl.Convolution2DFlipout(32, kernel_size=3, strides=(1,1), data_format=\"channels_last\", \n",
    "                                    padding=\"same\", activation=tf.nn.relu, name=\"conv_tfp_1b\", \n",
    "                                    kernel_divergence_fn=divergence_fn),\n",
    "    MaxPool2D(strides=(2,2), pool_size=(2,2), padding=\"same\"),\n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dense(512, activation='relu'),\n",
    "    \n",
    "    Dropout(0.2),\n",
    "    tfpl.DenseFlipout(units=5,activation='softmax', kernel_divergence_fn=divergence_fn),\n",
    "])\n",
    "model_bayes.summary()\n",
    "learning_rate = 1.0e-3\n",
    "\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    y_pred = tfp.distributions.MultivariateNormalTriL(y_pred)\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-fiber",
   "metadata": {},
   "source": [
    "# 4. Compile and Train the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class0-5 is used to track accuracy of different classes\n",
    "def class0(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 0), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class1(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 1), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class2(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 2), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class3(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 3), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "def class4(y_true, y_pred):\n",
    "    class_id_true = K.argmax(y_true, axis=-1)\n",
    "    class_id_preds = K.argmax(y_pred, axis=-1)\n",
    "    accuracy_mask = K.cast(K.equal(class_id_preds, 4), 'int32')\n",
    "    class_acc_tensor = K.cast(K.equal(class_id_true, class_id_preds), 'int32') * accuracy_mask\n",
    "    class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
    "    return class_acc\n",
    "\n",
    "model_bayes.compile(loss = negative_log_likelihood,\n",
    "              optimizer = Adam(learning_rate=learning_rate),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(),class0,class1,class2,class3,class4],\n",
    "              experimental_run_tf_function = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_bayes.fit(train_generator,\n",
    "                                steps_per_epoch = train_generator.samples // batch_size,\n",
    "                                validation_data = test_generator, \n",
    "                                validation_steps = test_generator.samples // batch_size,\n",
    "                                epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-cannon",
   "metadata": {},
   "source": [
    "# 5. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this section displays the plots of the model's accuracy and loss over the epochs\n",
    "np.savez(\"history_plots.npz\",history.history)\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(12, 3))\n",
    "fig.suptitle(\"Bayesian Model 2's accuracy and Loss\",fontsize=12)\n",
    "\n",
    "ax1.plot(history.history['categorical_accuracy'],label='train')\n",
    "ax1.plot(history.history['val_categorical_accuracy'],label='test')\n",
    "ax1.title.set_text('model accuracy')\n",
    "ax1.set_ylabel('accuracy')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['loss'],label='train')\n",
    "ax2.plot(history.history['val_loss'],label='test')\n",
    "ax2.title.set_text('model loss')\n",
    "ax2.set_ylabel('loss')\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function import and predict calcualtes and dispalys the prediction accuacy\n",
    "# Since our model provide the prediction in a distribution we take 300 samples form this distribution \n",
    "# we display all the samples in the 95 percnetile and ignore the rest ( the outliers)\n",
    "# the result is a bar, and the thickness of the bar represents the variance of prediction/ equivlent to the uncertainty\n",
    "\n",
    "def import_and_predict_bayes(image, true_label):\n",
    "    #read image\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    #show the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    img_resize = (cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC))/255.\n",
    "    predicted_probabilities = np.empty(shape=(300, 5))\n",
    "    predicted_probabilities2 = np.empty(shape=(300, 5))\n",
    "    predicted_probabilities3 = np.empty(shape=(300, 5))\n",
    "    for i in range(300):\n",
    "        predicted_probabilities2[i] =model_bayes.predict(img_resize[np.newaxis,...])   \n",
    "    pct_2p5 = np.array([np.percentile(predicted_probabilities2[:, i], 2.5) for i in range(5)])\n",
    "    pct_97p5 = np.array([np.percentile(predicted_probabilities2[:, i], 97.5) for i in range(5)])\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bar = ax.bar(np.arange(5), pct_97p5-0.02, color='red')\n",
    "    bar[true_label].set_color('green')\n",
    "    bar = ax.bar(np.arange(5), pct_2p5-0.02, color='white')\n",
    "    ax.set_xticklabels([''] + [x for x in label],fontsize=20)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_ylabel('Probability',fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "# the function import_and_predict2 similar to the previous function in calcuated the prediction accuracy \n",
    "# yet it takes T=10 samples form the prediction accuracy \n",
    "#it calcuates the variance \n",
    "# and form the variance it drives the two types of uncertainty (aleatoric and epistemic)    \n",
    "    \n",
    "def import_and_predict2(image_data, label, T):    \n",
    "    #read image\n",
    "    img = cv2.imread(image_data)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "    # resize and reshape the image\n",
    "    img_resize = (cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_CUBIC))/255.\n",
    "    img_reshape = img_resize[np.newaxis,...]\n",
    "    #predict the image\n",
    "    prediction = model_bayes.predict(img_reshape)    \n",
    "    p_hat=[]\n",
    "    for t in range (T):\n",
    "        p_hat.append(model_bayes.predict(img_reshape)[0])\n",
    "    p_hat=np.array(p_hat)\n",
    "    prediction = np.mean(p_hat, axis=0)\n",
    "    label_prediction = label[np.argmax(prediction)]\n",
    "    aleatoric = np.mean(p_hat*(1-p_hat), axis=0)\n",
    "    epistemic = np.mean(p_hat**2, axis=0) - np.mean(p_hat, axis=0)**2\n",
    "    label_prediction = [np.argmax(prediction)]\n",
    "    return prediction, label_prediction, aleatoric, epistemic\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and displaying the clcuated accurca\n",
    "label = os.listdir(root_dir)\n",
    "image0_dir = os.path.join(root_dir+'/0/20_right.jpeg')\n",
    "prediction = import_and_predict_bayes(image0_dir2,label.index('0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop passes over all images and calculates the accuracy and uncertinty\n",
    "input_dir=root_dir\n",
    "Class_Names=[\"0\",\"1\",\"2\",\"3\",\"4\"]\n",
    "total_prediction=np.empty(5)\n",
    "total_label_prediction=np.empty(1)\n",
    "total_aleatoric=np.empty(5)\n",
    "total_epistemic=np.empty(5)\n",
    "total_accuracy=np.empty(1)\n",
    "total_tracker=np.empty(1)\n",
    "tracker=np.empty(1)\n",
    "\n",
    "for Class in Class_Names:\n",
    "    path = os.path.join(input_dir,Class)\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        T=10 # T should be 2 or greater\n",
    "        image_dir=os.path.join(path,file)\n",
    "        prediction, label_prediction, aleatoric, epistemic=import_and_predict2(image_dir, label,T)                        \n",
    "        total_prediction=np.vstack((total_prediction,prediction))\n",
    "        total_label_prediction=np.vstack((total_label_prediction,label_prediction))\n",
    "        total_aleatoric=np.vstack((total_aleatoric,aleatoric))\n",
    "        total_epistemic=np.vstack((total_epistemic,epistemic))\n",
    "        tracker=Class_Names.index(Class)\n",
    "        total_tracker=np.vstack((total_tracker,tracker))   \n",
    "        if(int(\"\".join(map(str, label_prediction)))==Class_Names.index(Class)):\n",
    "                total_accuracy=np.vstack((total_accuracy,1))\n",
    "        else: \n",
    "                total_accuracy=np.vstack((total_accuracy,0))\n",
    "\n",
    "total_prediction1=total_prediction[1:,:]\n",
    "total_label_prediction1=total_label_prediction[1:,:]\n",
    "total_aleatoric1=total_aleatoric[1:,:]\n",
    "total_epistemic1=total_epistemic[1:,:]\n",
    "total_accuracy1=total_accuracy[1:,:]\n",
    "total_tracker1=total_tracker[1:,:]                \n",
    "np.savez(\"dataDR_bayes_700.npz\",total_prediction1,total_label_prediction1,total_aleatoric1,total_epistemic1,total_accuracy1,total_tracker1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
